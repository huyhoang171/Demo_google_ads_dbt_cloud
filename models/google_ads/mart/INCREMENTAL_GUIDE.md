# ðŸš€ Incremental Materialization Guide

This guide explains how the incremental strategy works in this project and how to use it effectively.

## ðŸ“‹ Overview

All **Fact Tables** (`fct_*`) now use **incremental materialization** with **merge strategy** to optimize:
- âš¡ Processing time
- ðŸ’° BigQuery costs
- ðŸ”„ Data freshness
- ðŸŽ¯ Late-arriving data handling

## ðŸ—ï¸ How It Works

### First Run (Initial Load)
```bash
dbt run --models fct_campaign_performance
```
- Creates table with **ALL historical data**
- Processes entire dataset from staging
- Takes longer but only happens once

### Subsequent Runs (Incremental)
```bash
dbt run --models fct_campaign_performance
```
- Only processes **last 3 days** of data
- Uses `MERGE` to update/insert records
- 10-100x faster than full refresh

### Logic Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Is this the first run?            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚
       Yes           No
        â”‚             â”‚
        v             v
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Full    â”‚   â”‚ Last 3  â”‚
  â”‚ History â”‚   â”‚ Days    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   MERGE     â”‚
        â”‚  Strategy   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”‘ Unique Keys

Each fact table has a composite `unique_key` to identify records:

| Model | Unique Key |
|-------|-----------|
| `fct_campaign_performance` | `[account_id, campaign_id, stat_date, ad_network_type, device]` |
| `fct_ad_group_performance` | `[account_id, campaign_id, ad_group_id, stat_date, ad_network_type, device]` |
| `fct_ad_performance` | `[account_id, campaign_id, ad_group_id, ad_id, stat_date, ad_network_type, device]` |
| `fct_keyword_performance` | `[account_id, campaign_id, ad_group_id, criterion_id, stat_date, ad_network_type, device]` |
| `fct_search_term_performance` | `[account_id, campaign_id, ad_group_id, search_term, stat_date, ad_network_type, device]` |

### How MERGE Works
```sql
-- If unique_key exists: UPDATE
-- If unique_key doesn't exist: INSERT

MERGE target_table AS target
USING new_data AS source
ON target.account_id = source.account_id
   AND target.campaign_id = source.campaign_id
   AND target.stat_date = source.stat_date
   -- ... other key columns
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT
```

## â° Lookback Window Configuration

### Default: 3-Day Lookback
The lookback window is **configurable** via `dbt_project.yml`:

```yaml
# dbt_project.yml
vars:
  incremental_lookback_days: 3  # Change this value as needed
```

```sql
-- In each model
{% if is_incremental() %}
    where stat_date >= date_sub(current_date(), 
        interval {{ var('incremental_lookback_days', 3) }} day)
{% endif %}
```

### How to Change

**Option 1: Permanently in dbt_project.yml**
```yaml
vars:
  incremental_lookback_days: 5  # Process 5 days instead of 3
```

**Option 2: One-time Override**
```bash
# Run with 7 days lookback
dbt run --models fct_* --vars '{"incremental_lookback_days": 7}'

# Run with 1 day lookback (faster, cheaper)
dbt run --models fct_* --vars '{"incremental_lookback_days": 1}'
```

**See [CONFIGURATION.md](../../../CONFIGURATION.md) for detailed configuration guide.**

### Why Lookback Window?
```sql
# Default: 3 days
{% if is_incremental() %}
    where stat_date >= date_sub(current_date(), interval {{ var('incremental_lookback_days', 3) }} day)
{% endif %}
```

**Reasons:**
1. **Late-arriving data**: Google Ads may update yesterday's data today
2. **Data corrections**: Conversions can be attributed retrospectively
3. **Safety buffer**: Ensures no data is missed
4. **Cost-effective**: Default 3 days is minimal overhead vs full history

**Configurable:** Change in `dbt_project.yml` or via command line.

### What Gets Processed (with default 3 days)

| Run Date | Data Processed | Records Updated |
|----------|----------------|-----------------|
| Jan 15 | Jan 13, 14, 15 | ~3 days Ã— campaigns Ã— devices |
| Jan 16 | Jan 14, 15, 16 | ~3 days Ã— campaigns Ã— devices |
| Jan 17 | Jan 15, 16, 17 | ~3 days Ã— campaigns Ã— devices |

**Change lookback:** See [CONFIGURATION.md](../../../CONFIGURATION.md) for choosing the right value.

## ðŸŽ¯ Common Commands

### Daily Run (Incremental)
```bash
# Run all fact tables (incremental mode)
dbt run --models fct_*

# Run specific fact table
dbt run --models fct_campaign_performance

# Run with selector
dbt run --select tag:fact
```

### Full Refresh
```bash
# Full refresh all fact tables
dbt run --models fct_* --full-refresh

# Full refresh specific model
dbt run --models fct_campaign_performance --full-refresh

# Full refresh entire mart layer
dbt run --models mart.* --full-refresh
```

### Test After Run
```bash
# Test data quality
dbt test --models fct_*

# Run and test together
dbt build --models fct_*
```

## ðŸ“Š Performance Comparison

### Before (Full Refresh - Table Materialization)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ fct_campaign_performance            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Processed: 365 days Ã— 100 campaigns
â”‚ Rows: ~36,500                       â”‚
â”‚ Time: 5-10 minutes                  â”‚
â”‚ Cost: $5-10 per run                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### After (Incremental - Merge Strategy)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ fct_campaign_performance            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Processed: 3 days Ã— 100 campaigns
â”‚ Rows: ~300                          â”‚
â”‚ Time: 30-60 seconds                 â”‚
â”‚ Cost: $0.10-0.50 per run           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Improvements:**
- âš¡ **100x faster** processing
- ðŸ’° **10-20x cheaper** per run
- ðŸ”„ **Same data freshness**
- ðŸŽ¯ **Better late-data handling**

## ðŸš¨ When to Full Refresh

### Required Full Refresh
- âœ… Initial project setup
- âœ… Schema changes to columns
- âœ… Changes to calculated metrics logic
- âœ… Migration from table to incremental

### Optional Full Refresh
- ðŸ”„ Data quality issues in historical data
- ðŸ”„ Backfilling after source data corrections
- ðŸ”„ Testing with clean slate

### Command
```bash
# Full refresh and test
dbt build --models fct_* --full-refresh

# Or use environment variable
DBT_FULL_REFRESH=1 dbt run --models fct_*
```

## ðŸ” Monitoring Incremental Runs

### Check Run Logs
```bash
# View run details
dbt run --models fct_campaign_performance --log-level debug

# Check for errors
grep "ERROR" logs/dbt.log
```

### Validate Data
```sql
-- Check for gaps in stat_date
SELECT 
    stat_date,
    COUNT(*) as record_count
FROM {{ ref('fct_campaign_performance') }}
WHERE stat_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
GROUP BY stat_date
ORDER BY stat_date DESC;

-- Check for duplicates (should be 0)
SELECT 
    account_id,
    campaign_id,
    stat_date,
    ad_network_type,
    device,
    COUNT(*) as duplicate_count
FROM {{ ref('fct_campaign_performance') }}
GROUP BY 1,2,3,4,5
HAVING COUNT(*) > 1;
```

## ðŸ› ï¸ Troubleshooting

### Issue: Data Not Updating
**Symptoms:** Yesterday's data not appearing

**Solution:**
```bash
# Check if data exists in staging
SELECT MAX(stat_date) FROM {{ ref('stg_google_ads__campaign_stats') }};

# Full refresh specific model
dbt run --models fct_campaign_performance --full-refresh
```

### Issue: Duplicate Records
**Symptoms:** Same record appearing multiple times

**Solution:**
1. Check unique_key configuration
2. Full refresh to clean
3. Add unique test:
```yaml
# schema.yml
models:
  - name: fct_campaign_performance
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - account_id
            - campaign_id
            - stat_date
            - ad_network_type
            - device
```

### Issue: High BigQuery Costs
**Symptoms:** Unexpected query costs

**Solution:**
```bash
# Check query size in logs
dbt run --models fct_* --log-level debug

# Reduce lookback window if needed (edit models)
where stat_date >= date_sub(current_date(), interval 2 day)  -- instead of 3

# Use partition filter in BigQuery (future enhancement)
```

## ðŸŽ¯ Best Practices

### 1. **Run Daily**
```bash
# Cron job / Scheduler
0 6 * * * cd /path/to/project && dbt run --models fct_*
```

### 2. **Monitor Data Quality**
```bash
dbt test --models fct_* --store-failures
```

### 3. **Use Selectors**
```yaml
# selectors.yml
selectors:
  - name: daily_refresh
    definition:
      union:
        - tag:fact
        - tag:report
```

```bash
dbt run --selector daily_refresh
```

### 4. **Separate Fact and Report Runs**
```bash
# Step 1: Incremental fact tables
dbt run --models fct_*

# Step 2: Full refresh report tables
dbt run --models rpt_*
```

### 5. **Document Model Dependencies**
```bash
# Generate lineage
dbt docs generate
dbt docs serve
```

## ðŸ“š Additional Resources

- [dbt Incremental Models](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/materializations#incremental)
- [BigQuery Merge Strategy](https://docs.getdbt.com/reference/resource-configs/bigquery-configs#merge-behavior-incremental-models)
- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)

## ðŸŽ“ Summary

âœ… **Fact tables** use `incremental` with `merge` strategy
âœ… **3-day lookback** handles late-arriving data
âœ… **Unique keys** prevent duplicates
âœ… **100x faster** than full refresh
âœ… **10-20x cheaper** per run
âœ… **Full refresh** available when needed

**Run command:**
```bash
# Daily incremental run
dbt run --models fct_*

# Weekly full refresh (optional)
dbt run --models fct_* --full-refresh
```
